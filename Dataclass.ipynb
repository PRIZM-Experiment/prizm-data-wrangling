{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ae71df",
   "metadata": {},
   "source": [
    "# SpectralData class tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55668ff3",
   "metadata": {},
   "source": [
    "This notebook is a simple demo to show how to use the SpectralData class in the prizmatoid module. This demo may change over demo as both the modules in the prizm-data-wrangling module change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873c42f",
   "metadata": {},
   "source": [
    "### Load the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e83a6b",
   "metadata": {},
   "source": [
    "comment: would be good to get to a state where we can add a setup.py file for easy install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1de63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prizmatoid as pzt\n",
    "import metadatabase as mdb\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "try:\n",
    "    reload(pzt)\n",
    "    reload(mdb)\n",
    "except:\n",
    "    from importlib import reload\n",
    "    reload(pzt)\n",
    "    reload(mdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87156dbb",
   "metadata": {},
   "source": [
    "### Load Data from ctimes 152440000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3af063",
   "metadata": {},
   "source": [
    "Comments: \n",
    "\n",
    "- No longer need to call data/patch directory variable in mdb. Might be good to see if that's actually the best way.\n",
    "- The class currently assumes you'll load data per antenna (and not two antennas simultaneously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebbb4e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files named `open.scio` could be found and/or read.\n",
      "No files named `open.scio` could be found and/or read.\n"
     ]
    }
   ],
   "source": [
    "data_directory = '../prizm_data/'\n",
    "patches_directory = './patches_data'\n",
    "\n",
    "\n",
    "ctime_intervals = [(152440000, 152450000)]\n",
    "components=['100MHz', 'switch']\n",
    "filters=['polarization_0']\n",
    "\n",
    "#Initialize an empty class that doesn't contain data\n",
    "data = pzt.SpectralData()\n",
    "#Access the load method to load data into the SpectralData class instance (without printing information)\n",
    "data.load_data(data_directory=data_directory, patches_directory=patches_directory, \n",
    "               ctime_intervals=ctime_intervals, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac8ab1",
   "metadata": {},
   "source": [
    "### Start operating on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb77cc",
   "metadata": {},
   "source": [
    "Comments:\n",
    "- trim_flags: applies the trim on all flags (if they exist), is it useful to have different sized trims per flag type?\n",
    "- find_entries: does what find_antenna_times and half of find_short100 do.\n",
    "- find_data_chunks: like the above a generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b96a816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate the flags (all flags are an optional parameter and are by default set to false, except for switch_flags)\n",
    "data.generate_flags(switch_flags=True)\n",
    "\n",
    "#Trim all flags (with default setting, 1 leading timestamp and 1 trailing timestamp)\n",
    "#Applied to all flags generated in the previous steps\n",
    "data.trim_flags(trim = (1,1))\n",
    "\n",
    "#Flag retrieval functions\n",
    "short_indices = data.find_entries(switch_type='short')\n",
    "start, end = data.find_data_chunks(switch_type='short')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b0ce8f",
   "metadata": {},
   "source": [
    "### Analysis or Wrangling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b651d",
   "metadata": {},
   "source": [
    "Below is a demo of the re-implementation of the get_short and the run_data functions + a vna_reading tool that is somewhat module-less "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4f030ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def read_vna_data(filename, delimiter=\",\", encoding=\"ISO-8859-1\"):\n",
    "    \"\"\"\n",
    "    Reads VNA measurements for efficiency calculation.\n",
    "    2017 - .csv with ; delimiters\n",
    "    2018 - .csv with , delimiters\n",
    "    2019 - .csv with , delimiters\n",
    "    2020 - Non-existent\n",
    "    2021 - .txt with \\t delimtiers\n",
    "\n",
    "    Data from 2018 are .csv with peculiarities handled by this file.\n",
    "    Data from 2022 are .txt with formatting as expected\n",
    "\n",
    "    Original function was written by Kelly A. Foran, adapted by Ronniy C. Joseph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename\n",
    "    startkey\n",
    "    delimiter\n",
    "    encoding\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Written by Kelly A. Foran\n",
    "    Adapted by Ronniy C. Joseph\n",
    "    \"\"\"\n",
    "    #Loop through file to figure out where the header starts, and how many data rows we're reading and create appropiate\n",
    "    #arrays\n",
    "    header_line, n_data_rows = find_start_and_end(filename)\n",
    "    data = np.zeros((n_data_rows, 6))\n",
    "\n",
    "    read_labels = False\n",
    "    read_data = False\n",
    "    data_counter = 0\n",
    "\n",
    "    with open(filename, 'r', encoding=encoding , errors = 'replace') as datafile:\n",
    "        for index, line in enumerate(datafile):\n",
    "\n",
    "            #Read VNA Measurement after the Column Header was read, see below.\n",
    "            if read_data:\n",
    "                data[data_counter] = decode_line(line, delimiter=delimiter, n_columns = len(column_header))\n",
    "                data_counter +=1\n",
    "\n",
    "            #Record Header Labels after the newline was found, see below.\n",
    "            if read_labels:\n",
    "                column_header = line.strip().split(delimiter)\n",
    "                read_labels = False\n",
    "                read_data = True\n",
    "\n",
    "            #Try and find the first newline if you haven't found it already\n",
    "            if read_labels != True and read_data != True:\n",
    "                tags = line.split(delimiter)\n",
    "                try:\n",
    "                    if tags[0] == '\\n':\n",
    "                        read_labels = True\n",
    "                except IndexError:\n",
    "                    pass\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_start_and_end(filename, lookup=\"\\n\", encoding=\"ISO-8859-1\"):\n",
    "    \"\"\"\n",
    "    This function reads a .txt file and returns the line where it finds the look-up character, and computes how many\n",
    "    lines to the end of file\n",
    "\n",
    "    It was built to scan Nivek's S11 VNA measurement outputs and figure out where the data actually starts so you\n",
    "    can ignore all the metadata\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    filename: str\n",
    "        path to text file\n",
    "\n",
    "    lookup: str\n",
    "        string that marks end of metadata\n",
    "\n",
    "    encoding: str\n",
    "        type of encoding (utf-8, etc.)\n",
    "\n",
    "    \"\"\"\n",
    "    header_line = None\n",
    "    with open(filename, encoding=encoding) as datafile:\n",
    "        line_counter = 0\n",
    "        for num, line in enumerate(datafile):\n",
    "            if lookup == line:\n",
    "                header_line = num + 1\n",
    "            line_counter += 1\n",
    "    try:\n",
    "        n_data_rows = line_counter - header_line - 1\n",
    "    except TypeError:\n",
    "        print(f\"Couldn't find look-up character {lookup}. Are you sure the file actually contains this? \")\n",
    "        raise\n",
    "    return header_line, n_data_rows\n",
    "\n",
    "\n",
    "def decode_line(line, delimiter, n_columns):\n",
    "    #Deals with formatting challenges across the various data sets\n",
    "    #The challenge is that frequency formatting changes based on year an delimiter, pending on channel spacing\n",
    "    #There are two cases\n",
    "    #All columns have two entries (pre- and post-comma)\n",
    "    #Or the Frequency channel only has 1 entry, because there is no decimal comma\n",
    "\n",
    "    tags = line.strip().split(delimiter)\n",
    "    empty_column = int((len(tags) - 1) / 2)\n",
    "    data = np.zeros(n_columns - 1)\n",
    "    tags = tags[:empty_column] + tags[(empty_column + 1):]\n",
    "    counter = 0\n",
    "\n",
    "    #print(tags)\n",
    "    #2021 VNA data, tab delimited data lines up with number of column labels\n",
    "    if len(tags) == 6:\n",
    "        for s, string in enumerate(tags):\n",
    "            data[s] = string.replace(\",\", \".\")\n",
    "        data = data.astype(float)\n",
    "\n",
    "    # <2021 data where every column has been split into before and after decimal points\n",
    "    elif len(tags) == 12:\n",
    "        indices = np.array([0,2,4,6,8,10])\n",
    "        for s in indices:\n",
    "            data[counter] = tags[s] + \".\" + tags[s+1]\n",
    "            counter += 1\n",
    "        data = data.astype(float)\n",
    "\n",
    "    #<2021 where frequencies are integers, but other columns have been split into two\n",
    "    #TODO build a robuster check there might be the unlikely odd case the Magnitude/Phase columns are ints\n",
    "    elif len(tags) == 10:\n",
    "        indices = np.array([0,1,3,5,6,8])\n",
    "        for s in indices:\n",
    "            if s == 0 or s == empty_column :\n",
    "                data[counter] = tags[s]\n",
    "            else:\n",
    "                data[counter] = tags[s] + \".\" + tags[s+1]\n",
    "            counter += 1\n",
    "        data = data.astype(float)\n",
    "    else:\n",
    "        raise Exception(\"A new case of VNA data formatting!\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_efficiency(freqs, s11_filename, xsmooth, delimiter='\\t'):\n",
    "    \"\"\" Finds the antenna efficiency from the antenna s11 only.\n",
    "        Reads VNA measurements for efficiency calculation.\n",
    "\n",
    "    2017 - .csv with ; delimiters\n",
    "    2018 - .csv with , delimiters\n",
    "    2019 - .csv with , delimiters\n",
    "    2020 - Non-existent\n",
    "    2021 - .txt with \\t delimtiers\n",
    "\n",
    "    Written by Kelly A. Foran\n",
    "    Adapted by Ronniy C. Joseph\n",
    "\n",
    "    \"\"\"\n",
    "    # read the s11 data\n",
    "    s11_data = read_vna_data(s11_filename, delimiter=delimiter)\n",
    "    s11_freqs = s11_data[:, 0]\n",
    "\n",
    "    # convert the s11 from dB to linear\n",
    "    s11_magnitude = 10 ** (s11_data[:, 1] / 20)\n",
    "\n",
    "    # interpolate and smooth efficiency to requested frequencies\n",
    "    #Define a Butterworth Smoothing kernel\n",
    "    sos = signal.butter(1, xsmooth, btype='lp', output='sos')\n",
    "    #Filter data and interpolate\n",
    "    lin = interp1d(s11_freqs, signal.sosfilt(sos, s11_magnitude), kind='slinear', fill_value=\"extrapolate\")\n",
    "    efficiency = 1 - (lin(freqs)) ** 2\n",
    "\n",
    "    return efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5116814",
   "metadata": {},
   "outputs": [],
   "source": [
    "vna_path = \"../prizm_data/prizm_vna_2019/antenna_s11_v2/\"\n",
    "s11_ew = vna_path + '100-1-ew-s11.csv'\n",
    "s11_ns = vna_path + '100-2-ns-s11.csv'\n",
    "\n",
    "\n",
    "freqs = np.linspace(0, 250000000, num=4096, endpoint=True)\n",
    "efficiency_smoothing = 0.02\n",
    "\n",
    "#This is an adaption of find_efficiency1, but slightly more robust against the horror that is data from 2017\n",
    "#But not quite suited for prizmatoid\n",
    "efficiency_ew = find_efficiency(freqs, s11_ew, efficiency_smoothing, delimiter=',')\n",
    "efficiency_ns = find_efficiency(freqs, s11_ns, efficiency_smoothing, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67e9c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "power0, power1 = data.compute_power(eff_ew=efficiency_ew, eff_ns=efficiency_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4d382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
